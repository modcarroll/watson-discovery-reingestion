{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "# Insert your project token block by clicking the vertical ellipses on the notebook and click \"Insert project token\"\n",
    "from project_lib import Project\n",
    "project = Project(project_id='{project-id}', project_access_token='{project-access-token}')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Discovery Service Document Reingestion\n",
    "By [Morgan Langlais](https://github.com/modlanglais/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ibm-watson\n",
    "!pip install bs4\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import DiscoveryV1\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import json\n",
    "import threading\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor as PoolExecutor\n",
    "import http.client\n",
    "import socket\n",
    "from ibm_watson import ApiException\n",
    "\n",
    "beginTime = time.time()\n",
    "\n",
    "# @hidden_cell\n",
    "# Insert the appropriate Discovery credentials here\n",
    "environmentId = \"{environment-id}\"\n",
    "collectionId = \"{collection-id}\"\n",
    "discovery = DiscoveryV1(\n",
    "    '2019-04-30',\n",
    "    iam_apikey=\"{api-key}\")\n",
    "\n",
    "collection = discovery.get_collection(environmentId, collectionId).get_result()\n",
    "totalDocuments = collection['document_counts']['available']\n",
    "print(\"**Total number of documents in collection \" + collectionId + \": \" + str(totalDocuments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section gets a list of all the document IDs in a given collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmap_helper(fn, output_list, input_list, i):\n",
    "    output_list[i] = fn(input_list[i])\n",
    "\n",
    "def pmap(fn, input):\n",
    "    input_list = list(input)\n",
    "    output_list = [None for _ in range(len(input_list))]\n",
    "    threads = [threading.Thread(target=pmap_helper,\n",
    "                                args=(fn, output_list, input_list, i),\n",
    "                                daemon=True)\n",
    "               for i in range(len(input_list))]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return output_list\n",
    "\n",
    "def all_document_ids(discovery,\n",
    "                     environmentId,\n",
    "                     collectionId):\n",
    "    \"\"\"\n",
    "    Return a list of all of the document ids found in a\n",
    "    Watson Discovery collection.\n",
    "\n",
    "    The arguments to this function are:\n",
    "    discovery      - an instance of DiscoveryV1\n",
    "    environment_id - an environment id found in your Discovery instance\n",
    "    collection_id  - a collection id found in the environment above\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    alphabet = \"0123456789abcdef\"   # Hexadecimal digits, lowercase\n",
    "    chunk_size = 10000\n",
    "\n",
    "    def maybe_some_ids(prefix):\n",
    "        \"\"\"\n",
    "        A helper function that does the query and returns either:\n",
    "        1) A list of document ids\n",
    "        2) The `prefix` that needs to be subdivided into more focused queries\n",
    "        \"\"\"\n",
    "        need_results = True\n",
    "        while need_results:\n",
    "            try:\n",
    "                response = discovery.query(environmentId,\n",
    "                                           collectionId,\n",
    "                                           count=chunk_size,\n",
    "                                           filter=\"extracted_metadata.sha1::\"\n",
    "                                           + prefix + \"*\",\n",
    "                                           return_fields=\"extracted_metadata.sha1\").get_result()\n",
    "                need_results = False\n",
    "            except Exception as e:\n",
    "                print(\"will retry after error\", e)\n",
    "\n",
    "        if response[\"matching_results\"] > chunk_size:\n",
    "            return prefix\n",
    "        else:\n",
    "            return [item[\"id\"] for item in response[\"results\"]]\n",
    "\n",
    "    prefixes_to_process = [\"\"]\n",
    "    while prefixes_to_process:\n",
    "        prefix = prefixes_to_process.pop(0)\n",
    "        prefixes = [prefix + letter for letter in alphabet]\n",
    "        # `pmap` here does the requests to Discovery concurrently to save time.\n",
    "        results = pmap(maybe_some_ids, prefixes)\n",
    "        for result in results:\n",
    "            if isinstance(result, list):\n",
    "                doc_ids += result\n",
    "            else:\n",
    "                prefixes_to_process.append(result)\n",
    "\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "allDocIds = all_document_ids(discovery,\n",
    "                           environmentId,\n",
    "                           collectionId)\n",
    "    \n",
    "df = pandas.DataFrame(allDocIds)\n",
    "projectFileName = 'allDocIds-' + collectionId +  '.csv'\n",
    "project.save_data(projectFileName, df.to_csv(header=None, index=None), set_project_asset=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterates through each document in the collection and transforms the document according to the block of code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To iterate over every collection in an environment, uncomment and wrap the below code in this:\n",
    "# allCollections = discovery.list_collections(environmentId).get_result()['collections']\n",
    "# for collection in allCollections:\n",
    "    # collectionId = collection['collection_id']\n",
    "    # Move the code block below to here, be mindful of indentions to keep inside this loop\n",
    "    \n",
    "#################################################\n",
    "startTime = time.time()\n",
    "counter = 0\n",
    "filesNotAdded = ['These documents encountered an error during the document transformation process. Note there may be duplicates. See notebook for more information.']\n",
    "\n",
    "def doStuff(documentId):\n",
    "    global counter\n",
    "    counter = counter + 1\n",
    "    print(\"Starting (\" + str(counter) + \"/\" + str(len(allDocIds)) + \")...\")\n",
    "    filterId = '_id:' + documentId\n",
    "\n",
    "    # 1.) Get document from Discovery collection\n",
    "    try:\n",
    "        discQuery = discovery.query(environmentId, collectionId, filter=filterId).get_result()['results'][0]\n",
    "    except ApiException as ex:\n",
    "        print(\"Some error occured on document #\" + str(documentId))\n",
    "        print(\"Query failed with status code \" + str(ex.code) + \": \" + ex.message)\n",
    "        filesNotAdded.append(documentId + \": \" + str(ex.code) + \" \" + str(ex.message))\n",
    "\n",
    "    filename = discQuery['extracted_metadata']['filename']\n",
    "    filetype = discQuery['extracted_metadata']['file_type']\n",
    "\n",
    "    #****************TRANSFORM HERE****************#\n",
    "    # Make modifications to change your document however you would like.\n",
    "    transformedDoc = discQuery\n",
    "    # In this case, removing the metadata field\n",
    "    if 'metadata' in transformedDoc:\n",
    "        metadatafield = transformedDoc['metadata']\n",
    "        for field in metadatafield:\n",
    "            transformedDoc[field] = metadatafield[field]\n",
    "        del transformedDoc['metadata']\n",
    "\n",
    "  #******************END TRANSFORM******************#\n",
    "\n",
    "    # This can be deleted from the data assets AFTER processing the notebook is completed. It will be re-created with each run.\n",
    "    projectFileName = 'doNotDelete' + collectionId + '.json'\n",
    "    project.save_data(projectFileName, json.dumps(transformedDoc), set_project_asset=True, overwrite=True)\n",
    "\n",
    "    # 3.) Push updated document to Discovery.\n",
    "    try:\n",
    "        discUpdate = discovery.update_document(environmentId, collectionId, documentId, file=project.get_file(projectFileName), filename=filename, accept_json=True).get_result()\n",
    "    except ApiException as ex:\n",
    "        print(\"Some error occured on document #\" + str(documentId))\n",
    "        print(\"Update Document failed with status code \" + str(ex.code) + \": \" + ex.message)\n",
    "        filesNotAdded.append(documentId + \": \" + str(ex.code) + \" \" + str(ex.message))\n",
    "        \n",
    "with PoolExecutor(max_workers=32) as executor:\n",
    "    for _ in executor.map(doStuff, allDocIds):\n",
    "        pass\n",
    "    \n",
    "dataf = pandas.DataFrame(filesNotAdded)\n",
    "projectNotAddedFileName = 'FilesNotAdded' + collectionId + '.csv'\n",
    "project.save_data(projectNotAddedFileName, dataf.to_csv(header=None, index=None), set_project_asset=True, overwrite=True)\n",
    "#################################################\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"Updating \" + str(len(allDocIds)) + \" documents took \" + str(endTime - startTime) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get each document status\n",
    "#### Note: Filename may appear as `null` in the results, but this is normal and expected as the Discovery .update() function takes some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentSuccesses = ['Successfully updated']\n",
    "documentFailures = ['Failed to update']\n",
    "\n",
    "def getStatus(documentId):\n",
    "    filterId = '_id:' + documentId\n",
    "    try:\n",
    "        discQueryStatus = discovery.get_document_status(environmentId, collectionId, documentId).get_result()\n",
    "    except ApiException as ex:\n",
    "        print(\"Query failed with status code \" + str(ex.code) + \": \" + ex.message)\n",
    "        \n",
    "    if discQueryStatus['status'] == 'failed':\n",
    "        print (documentId + \"failure. See Discovery UI for details.\")\n",
    "        documentFailures.append(documentId)\n",
    "    else:\n",
    "        documentSuccesses.append(documentId)\n",
    "        \n",
    "with PoolExecutor(max_workers=16) as executor:\n",
    "    for _ in executor.map(getStatus, allDocIds):\n",
    "        pass\n",
    "        \n",
    "dataf = pandas.DataFrame(documentFailures)\n",
    "datas = pandas.DataFrame(documentSuccesses)\n",
    "projectSuccFileName = 'DocumentSuccess' + collectionId + '.csv'\n",
    "projectFailFileName = 'DocumentFail' + collectionId + '.csv'\n",
    "project.save_data(projectSuccFileName, datas.to_csv(header=None, index=None), set_project_asset=True, overwrite=True)\n",
    "project.save_data(projectFailFileName, dataf.to_csv(header=None, index=None), set_project_asset=True, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
