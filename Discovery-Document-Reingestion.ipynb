{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id={'project-id'}, project_access_token={'project-access-token'})\n",
    "pc = project.project_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Discovery Service Document Reingestion\n",
    "By [Morgan Langlais](https://github.com/modlanglais/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ibm-watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson import DiscoveryV1\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import json\n",
    "import threading\n",
    "\n",
    "# @hidden_cell\n",
    "# Insert the appropriate Discovery credentials here\n",
    "environmentId = {'environment-id'}\n",
    "collectionId = {'collection-id'}\n",
    "discovery = DiscoveryV1(\n",
    "    {'version'},\n",
    "    iam_apikey={'iam-apikey'})\n",
    "\n",
    "collection = discovery.get_collection(environmentId, collectionId).get_result()\n",
    "totalDocuments = collection['document_counts']['available']\n",
    "print(\"**Total number of documents in collection \" + collectionId + \": \" + str(totalDocuments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section gets a list of all the document IDs in a given collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmap_helper(fn, output_list, input_list, i):\n",
    "    output_list[i] = fn(input_list[i])\n",
    "\n",
    "def pmap(fn, input):\n",
    "    input_list = list(input)\n",
    "    output_list = [None for _ in range(len(input_list))]\n",
    "    threads = [threading.Thread(target=pmap_helper,\n",
    "                                args=(fn, output_list, input_list, i),\n",
    "                                daemon=True)\n",
    "               for i in range(len(input_list))]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    return output_list\n",
    "\n",
    "def all_document_ids(discovery,\n",
    "                     environmentId,\n",
    "                     collectionId):\n",
    "    \"\"\"\n",
    "    Return a list of all of the document ids found in a\n",
    "    Watson Discovery collection.\n",
    "\n",
    "    The arguments to this function are:\n",
    "    discovery      - an instance of DiscoveryV1\n",
    "    environment_id - an environment id found in your Discovery instance\n",
    "    collection_id  - a collection id found in the environment above\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    alphabet = \"0123456789abcdef\"   # Hexadecimal digits, lowercase\n",
    "    chunk_size = 10000\n",
    "\n",
    "    def maybe_some_ids(prefix):\n",
    "        \"\"\"\n",
    "        A helper function that does the query and returns either:\n",
    "        1) A list of document ids\n",
    "        2) The `prefix` that needs to be subdivided into more focused queries\n",
    "        \"\"\"\n",
    "        need_results = True\n",
    "        while need_results:\n",
    "            try:\n",
    "                response = discovery.query(environmentId,\n",
    "                                           collectionId,\n",
    "                                           count=chunk_size,\n",
    "                                           filter=\"extracted_metadata.sha1::\"\n",
    "                                           + prefix + \"*\",\n",
    "                                           return_fields=\"extracted_metadata.sha1\").get_result()\n",
    "                need_results = False\n",
    "            except Exception as e:\n",
    "                print(\"will retry after error\", e)\n",
    "\n",
    "        if response[\"matching_results\"] > chunk_size:\n",
    "            return prefix\n",
    "        else:\n",
    "            return [item[\"id\"] for item in response[\"results\"]]\n",
    "\n",
    "    prefixes_to_process = [\"\"]\n",
    "    while prefixes_to_process:\n",
    "        prefix = prefixes_to_process.pop(0)\n",
    "        prefixes = [prefix + letter for letter in alphabet]\n",
    "        # `pmap` here does the requests to Discovery concurrently to save time.\n",
    "        results = pmap(maybe_some_ids, prefixes)\n",
    "        for result in results:\n",
    "            if isinstance(result, list):\n",
    "                doc_ids += result\n",
    "            else:\n",
    "                prefixes_to_process.append(result)\n",
    "\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "allDocIds = all_document_ids(discovery,\n",
    "                           environmentId,\n",
    "                           collectionId)\n",
    "\n",
    "for doc_id in allDocIds:\n",
    "    print(doc_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterates through each document in the collection and transforms the document according to the block of code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for documentId in allDocIds:\n",
    "    print(\"**Updating document #\" + documentId + \"...\")\n",
    "    print(\"(\" + str(allDocIds.count(documentId)) + \"/\" + str(len(allDocIds)) + \")\")\n",
    "    filterId = '_id:' + documentId\n",
    "    \n",
    "    # 1.) Get document from Discovery collection\n",
    "    discQuery = discovery.query(environmentId, collectionId, filter=filterId).get_result()['results'][0]\n",
    "    # print(\"**Document #\" + documentId + \" query results: \")\n",
    "    # print(json.dumps(discQuery, indent=2))\n",
    "    filename = discQuery['extracted_metadata']['filename']\n",
    "    filetype = discQuery['extracted_metadata']['file_type']\n",
    "    \n",
    "    #****************TRANSFORM HERE****************#\n",
    "    # 2.) Apply changes to your document however you would like. In this example, I move metadata fields up one level\n",
    "    transformedDoc = discQuery\n",
    "    if 'metadata' in transformedDoc:\n",
    "        metadatafield = transformedDoc['metadata']\n",
    "        for field in metadatafield:\n",
    "            transformedDoc[field] = metadatafield[field]\n",
    "        del transformedDoc['metadata']\n",
    "        \n",
    "    # Deleting reserved, Watson-populated fields\n",
    "    del transformedDoc['id']\n",
    "    del transformedDoc['result_metadata']\n",
    "    \n",
    "    print(\"**Transformed document: \")\n",
    "    print(json.dumps(transformedDoc, indent=2))\n",
    "  #******************END TRANSFORM******************#\n",
    "    \n",
    "    projectFileName = 'currentDiscoveryDoc.json'\n",
    "    project.save_data(projectFileName, json.dumps(transformedDoc), set_project_asset=True, overwrite=True)\n",
    "    \n",
    "    # 3.) Push updated document to Discovery.\n",
    "    discUpdate = discovery.update_document(environmentId, collectionId, documentId, file=project.get_file(projectFileName), filename=filename, accept_json=True).get_result()\n",
    "    # print(json.dumps(discUpdate, indent=2))\n",
    "    \n",
    "    print(\"**Finished updating document #\" + documentId + \"...\")\n",
    "    print()\n",
    "    print(\"**************************************************************************\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get each document status\n",
    "#### Note: Filename may appear as `null` in the results, but this is normal and expected as the Discovery .update() function takes some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for documentId in allDocIds:\n",
    "    print(\"**After Reingestion Document Status: \")\n",
    "    filterId = '_id:' + documentId\n",
    "    discQueryStatus = discovery.get_document_status(environmentId, collectionId, documentId).get_result()\n",
    "    print(json.dumps(discQueryStatus, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = discovery.get_collection(environmentId, collectionId).get_result()\n",
    "totalDocuments = collection['document_counts']['available']\n",
    "print(\"**Total number of documents in collection \" + collectionId + \": \" + str(totalDocuments))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
